<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="page" content="blog">
    <link rel="stylesheet" href="styles.css"/>
    <title>Andreas Moe</title>
    <script src="header.js"></script>
</head>
<body>
<div id="header"></div>
<h2 id="blogtitle">My thuoughts on "Alignment faking in large language models" by Greenblatt et. al.</h2>
<p>
    The paper can be found <a href="https://arxiv.org/abs/2412.14093">here</a>.
    The introduction is hard to read. They present some results, but it's unclear what they've measured.
    It's not that surprising that the model is alignment faking in the prompted case.
    Sensitivity analysis is not clear enough. It's hard to tell which variations cause changes, and which are statistically significant.
    It takes mental effort to find the large deviations. I think these should have been presented on their own.

</p>

</body>
</html>